{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    2015 11 30 Classe utilizzata per esplorare il dataset coop su cui fare analisi temporale\n",
    "    \n",
    "    1: Estrazione dal json dei dati temporali\n",
    "    2: Calcolo distribuzioni\n",
    "    3: Calcolo entropia annuale\n",
    "    4: Outlier detection\n",
    "    5: Entropia pesata su dati filtrati con 4\n",
    "    6: Calcolo traiettorie di spesa\n",
    "    7: Clustering traiettorie di spesa e ricerca outlier\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  1: Estrazione dal json dei dati temporali\n",
    "# lettura file json e  estrazione della componenente temporale degli acquisti\n",
    "# esportazione in un nuovo dataframe per calcolare le statistiche\n",
    "\n",
    "import gzip\n",
    "import json\n",
    "infile = '../data/dataset_shop70_coopometro.json.gz'\n",
    "fI = gzip.open(infile)\n",
    "\n",
    "outfile='../data/dataset_shop70_coopometro_temporal_part.csv.gz'\n",
    "fO = gzip.open(outfile, 'wb')\n",
    "fO.write(\"customer_id;scontrino_id;anno;mese;giorno;ora;giorno_settimana;settimana_anno;quantita;importo\\n\")\n",
    "for rec in fI:\n",
    "    customer=json.loads(rec)\n",
    "    for key in customer['data']:\n",
    "        fO.write(\"%s;%s;%s;%s;%s;%s;%s;%s;%s;%s\\n\" %(customer['customer_id'],customer['data'][key]['scontrino_id'],\n",
    "                                                    customer['data'][key]['anno'],\n",
    "                                                    customer['data'][key]['mese_n'],\n",
    "                                                     customer['data'][key]['giorno_n'],\n",
    "                                                     customer['data'][key]['ora'],\n",
    "                                                     customer['data'][key]['giorno_settimana_n'],\n",
    "                                                     customer['data'][key]['settimana_anno'],\n",
    "                                                     int(sum([v[0] for v in customer['data'][key]['basket'].values()])),\n",
    "                                                     int(sum([v[1] for v in customer['data'][key]['basket'].values()])),\n",
    "                                                    ))\n",
    "    fO.flush()\n",
    "fO.flush()\n",
    "fO.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#caricamento dataframe di cui sopra\n",
    "import pandas as pd\n",
    "outfile='../data/dataset_shop70_coopometro_temporal_part.csv.gz'\n",
    "df=pd.read_csv(outfile, compression='gzip', sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  2: Calcolo distribuzioni\n",
    "#distribuzioni scontrino\n",
    "import pylab\n",
    "path='./'\n",
    "for col in list(df.columns)[2:-2]:\n",
    "    s=pd.DataFrame(df.groupby([col])['scontrino_id'].count())\n",
    "    s.plot(kind='bar')\n",
    "    pylab.xticks(rotation=0)\n",
    "    pylab.xlabel(col, fontsize=16)\n",
    "    pylab.ylabel(\"#scontrini\",fontsize=16)\n",
    "    pylab.savefig(\"%s%s.png\" %(path,col))\n",
    "    pylab.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  2: Calcolo distribuzioni\n",
    "#distribuzioni quantita\n",
    "import pylab\n",
    "path='./'\n",
    "for attr_raggr in (['quantita','importo']):\n",
    "    for col in list(df.columns)[2:-2]:\n",
    "        s=pd.DataFrame(df.groupby([col])[attr_raggr].sum())\n",
    "        s.plot(kind='bar')\n",
    "        if len(s)>10:\n",
    "            locs, labels = pylab.xticks()\n",
    "            pylab.xticks(locs[::5], list(s.ix[:,0][::5].index))\n",
    "        pylab.xticks(rotation=0)\n",
    "        pylab.xlabel(col, fontsize=16)\n",
    "        pylab.ylabel(\"#oggetti acquistati\",fontsize=16)\n",
    "        pylab.savefig(\"%s%s_%s.png\" %(path,col,attr_raggr))\n",
    "        pylab.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  2: Calcolo distribuzioni\n",
    "#count dei momenti distinti\n",
    "import pylab\n",
    "path='./'\n",
    "for col in list(df.columns)[3:-2]:\n",
    "    df_distinct=pd.DataFrame(df.groupby(['customer_id','anno'])[col].apply(lambda x: len(x.unique()))).reset_index()\n",
    "    df_distinct=pd.DataFrame(df_distinct.groupby([col])['customer_id'].count()  )\n",
    "    df_distinct.plot(kind='bar')\n",
    "    pylab.xticks(rotation=0)\n",
    "    pylab.ylabel('#customers',fontsize=16)\n",
    "    pylab.xlabel(col,fontsize=16)\n",
    "    pylab.savefig(\"%s%s_distinct.png\" %(path,col))\n",
    "    pylab.clf()\n",
    "pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  2: Calcolo distribuzioni\n",
    "#distribuzione quantita e import\n",
    "import pylab\n",
    "path='./'\n",
    "for col in list(df.columns)[-2:]:\n",
    "    df[col].hist(bins=100)\n",
    "    pylab.yscale('log')\n",
    "    pylab.ylabel('#scontrini',fontsize=16)\n",
    "    pylab.xlabel(col,fontsize=16)\n",
    "    pylab.savefig(\"%s%s.png\" %(path,col))\n",
    "    pylab.clf()\n",
    "pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  2: Calcolo distribuzioni\n",
    "# spesa media\n",
    "df['media']=df['importo']/df['quantita']\n",
    "\n",
    "path='./'\n",
    "df['media'].hist(bins=300)\n",
    "pylab.yscale('log')\n",
    "pylab.ylabel('#scontrini',fontsize=16)\n",
    "pylab.xlabel('importo/quantita')\n",
    "pylab.savefig(\"%simporto_diviso_quantita.png\" %(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  2: Calcolo distribuzioni\n",
    "#distribuzione delle mediane\n",
    "import pylab\n",
    "import datetime\n",
    "path='./'\n",
    "vals={'media':[],'quantita':[],'importo':[]}\n",
    "users=list(df['customer_id'].unique())\n",
    "i=0\n",
    "for uid in users:\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print \"%s: %s users\" %(datetime.datetime.now(),i)\n",
    "    curr_user=df[df['customer_id']==uid]\n",
    "    for col in ['media','quantita','importo']:\n",
    "        vals[col].append(list(curr_user.sort(col)[col])[int(len(curr_user)/2)])\n",
    "\n",
    "for key in vals:\n",
    "    pylab.hist(vals[key],100)\n",
    "    pylab.ylabel('#scontrini',fontsize=16)\n",
    "    pylab.xlabel('%s' %(key), fontsize=16)\n",
    "    pylab.savefig(\"%s%s_%s.png\" %(path,key,'mediana'))\n",
    "    pylab.clf()\n",
    "mediane=pd.DataFrame(vals).to_csv('../data/dataset_shop70_coopometro.json.mediana_acquisti') #manca id della persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3: Calcolo entropia annuale\n",
    "import datetime\n",
    "df_entropy=pd.DataFrame()\n",
    "users=list(df['customer_id'].unique())\n",
    "cols=list(df.columns[3:-2])\n",
    "i=0\n",
    "print \"%s: start\" %(datetime.datetime.now())\n",
    "for cus in users:\n",
    "    curr_user=df[df['customer_id']==cus].copy()\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print \"%s: performed %s users\" %(datetime.datetime.now(),i)\n",
    "        \n",
    "    for anno in np.arange(2007,2015):\n",
    "        dfs=[]\n",
    "        for col in cols:\n",
    "            dfs.append(pd.DataFrame.from_dict({cus:get_entropy(curr_user[curr_user['anno']==anno],col)},orient='index'))\n",
    "        dfs.append(pd.DataFrame.from_dict({cus:{'anno':anno}},orient='index')) #join dei df dello stesso anno \n",
    "        df_entropy=df_entropy.append(pd.concat(dfs,axis=1)) #aggiunta del df dell'anno corrente agli altri anni\n",
    "df_entropy.to_csv('../data/dataset_shop70_coopometro.json.entropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_entropy (curr_user,col):\n",
    "    \"\"\"\n",
    "        Calcola l'entropia per il l'utente corrente per la colonna col\n",
    "        :param curr_user dataframe:dataframe dell'utente\n",
    "        :col text: colonna su cui calcolare l'entropia\n",
    "        :return d dict: dizionario {col:entropy}\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    s=pd.DataFrame(curr_user.groupby([col])['scontrino_id'].count())\n",
    "    s['tot']=[sum(s['scontrino_id'])]*len(s)\n",
    "    if col=='giorno':\n",
    "        num_classi=31\n",
    "    elif col=='mese':\n",
    "        num_classi=12\n",
    "    elif col=='ora':\n",
    "        num_classi=14\n",
    "    elif col=='giorno_settimana':\n",
    "        num_classi=7\n",
    "    elif col=='settimana_anno':\n",
    "        num_classi=53\n",
    "    return {col:(-1*sum(s['scontrino_id']/s['tot']*np.log(s['scontrino_id']/s['tot']))/np.log(num_classi))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#statistica sulla entropia\n",
    "import pylab\n",
    "path='./'\n",
    "#tolgo i casi in cui ho zero in tutte le entropie perchè non avevo dati\n",
    "df_entropy=df_entropy[(df_entropy['mese']<>0) & (df_entropy['giorno']<>0)& (df_entropy['ora']<>0)& (df_entropy['giorno_settimana']<>0) & (df_entropy['settimana_anno']<>0) ]\n",
    "for col in list(df_entropy.columns)[0:-1]:\n",
    "    df_entropy[col].hist()\n",
    "    pylab.xticks(rotation=0)\n",
    "    pylab.title(col,fontsize=20)\n",
    "    pylab.xlabel(\"entropy\", fontsize=16)\n",
    "    pylab.ylabel(\"#users\",fontsize=16)\n",
    "    pylab.savefig(\"%s%s_entropy.png\" %(path,col))\n",
    "    pylab.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4 Eliminazione outlier usando MAD e IQR\n",
    "import gzip\n",
    "import datetime\n",
    "\n",
    "#nuovi file\n",
    "f_iqr=gzip.open('../data/dataset_shop70_coopometro_temporal_part_filtered_iqr.csv.gz','w')\n",
    "f_mad=gzip.open('../data/dataset_shop70_coopometro_temporal_part_filtered_mad.csv.gz','w')\n",
    "f_iqr.write(';'.join(list(df.columns))+\"\\n\")\n",
    "f_mad.write(';'.join(list(df.columns))+\"\\n\")\n",
    "\n",
    "#file con statistiche\n",
    "f_filter_stats=open('../data/dataset_shop70_coopometro_temporal_part_filtered_stats.csv','w')\n",
    "f_filter_stats.write(\"customer_id;filterd_iqr;filtered_mad\\n\")\n",
    "\n",
    "i=0\n",
    "for uid in users:\n",
    "    i+=1\n",
    "    if i%1000==0:\n",
    "        print \"%s: users %s\" %(datetime.datetime.now(),i)\n",
    "    curr_user=df[df['customer_id']==uid]\n",
    "    curr_user_mad=curr_user.copy()\n",
    "    df_stat=curr_user[['customer_id','quantita','importo']].describe()\n",
    "    init_cols=list(curr_user.columns)\n",
    "    init_rows=len(curr_user)\n",
    "    \n",
    "    #aggiungiamo le colonne iqr e mad e i quartili\n",
    "    for col in ['quantita','importo']:\n",
    "        curr_user['IQR_%s'%(col)]=[df_stat[col].ix['75%']-df_stat[col].ix['25%']]*df_stat.ix['count'][0]\n",
    "        curr_user['Q1_%s'%(col)]=[df_stat[col].loc['25%']]*df_stat.ix['count'][0]\n",
    "        curr_user['Q3_%s'%(col)]=[df_stat[col].loc['75%']]*df_stat.ix['count'][0]\n",
    "\n",
    "    #filtro outlier con     #IQR\n",
    "    for col in ['quantita','importo']:\n",
    "        curr_user=curr_user[curr_user['Q1_%s' %(col)]-1.5*curr_user['IQR_%s' %(col)]<=curr_user[col]]\n",
    "        curr_user=curr_user[curr_user[col]<=curr_user['Q3_%s'%(col)]+1.5*curr_user['IQR_%s'%(col)]]\n",
    "\n",
    "    #salvare risultato del filtro di iqr\n",
    "    curr_user[init_cols].to_csv(f_iqr,header=False,sep=';',index=False)\n",
    "    nrows_iqr=init_rows-len(curr_user)\n",
    "\n",
    "    #MAD\n",
    "    curr_user=curr_user_mad\n",
    "    for col in ['quantita','importo']:\n",
    "        res=sorted(abs(curr_user[col]-df_stat[col].loc['50%']))\n",
    "        mad=res[int(len(res)/2)]\n",
    "        curr_user=curr_user[curr_user[col]<=(df_stat[col].ix['50%']+3*mad*1.483)]\n",
    "        curr_user=curr_user[curr_user[col]>=(df_stat[col].ix['50%']-3*mad*1.483)]\n",
    "\n",
    "    #salvare risultato del filtro di mad\n",
    "    curr_user.to_csv(f_mad,header=False,sep=';',index=False)\n",
    "\n",
    "    #salvo risulato delle statistiche di eliminaizone\n",
    "    f_filter_stats.write('%s;%s;%s\\n' %(uid,nrows_iqr,(init_rows-len(curr_user))))\n",
    "    \n",
    "    f_iqr.flush()\n",
    "    f_mad.flush()\n",
    "    f_filter_stats.flush()\n",
    "\n",
    "#chiudo output stream    \n",
    "f_iqr.flush()\n",
    "f_iqr.close()\n",
    "f_mad.flush()\n",
    "f_mad.close()\n",
    "f_filter_stats.flush()\n",
    "f_filter_stats.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#statistica sul numero di dati cancellati\n",
    "filter_file_stats='../data/dataset_shop70_coopometro_temporal_part_filtered_stats.csv'\n",
    "df_fs=pd.read_csv(filter_file_stats, sep=';')\n",
    "count_scontrini_utente=pd.DataFrame(df.groupby(['customer_id'])['scontrino_id'].count()) #num righe per utente\n",
    "df_fs.index=df_fs['customer_id']\n",
    "del(df_fs['customer_id'])\n",
    "df_fs=pd.concat([df_fs,count_scontrini_utente],axis='inner') #non avevo salvato il totale righe per utente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_fs['perc_iqr']=df_fs['filterd_iqr']/df_fs['scontrino_id']\n",
    "df_fs['perc_mad']=df_fs['filtered_mad']/df_fs['scontrino_id']\n",
    "df_fs['perc_iqr'].hist(bins=np.arange(0,0.4,.05),alpha=0.50, color='r',label='IQR')\n",
    "df_fs['perc_mad'].hist(bins=np.arange(0,0.4,.05),alpha=0.50, color='b',label='MAD')\n",
    "pylab.xlabel('percentuale dati filtrati', fontsize=16)\n",
    "pylab.ylabel('#customer_id', fontsize=16)\n",
    "pylab.legend()\n",
    "pylab.savefig('../experiments/20151130_analisi_esplorativa/distribuzione_dati_filtrati.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stat sul dato iqr ottenuto filtrando con il codice due blocchi sopra\n",
    "import pandas as pd\n",
    "input_file='../data/dataset_shop70_coopometro_temporal_part_filtered_iqr.csv.gz'\n",
    "df_iqr=pd.read_csv(input_file, compression='gzip',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#definizione del processo parallelo per calcolare entropia pesata sui dati filtrati\n",
    "from IPython.parallel import Client\n",
    "\n",
    "def parallel_plot(view, users,col2):\n",
    "    import numpy as np\n",
    "    results=[]\n",
    "    for uids,i in zip(users,np.arange(0,len(users))):\n",
    "        results.append(view.apply(plot, uids,i,col2))\n",
    "    \n",
    "    return [r.result for r in results]\n",
    "\n",
    "def plot(users,i,col2):\n",
    "    \n",
    "    def get_wheighted_entropy (curr_user,col,col2):\n",
    "        \"\"\"\n",
    "            Calcola l'entropia per il l'utente corrente per la colonna col\n",
    "            :param curr_user dataframe:dataframe dell'utente\n",
    "            :param col text: colonna su cui calcolare l'entropia\n",
    "            :param col2 text: colonna per cui sommare\n",
    "            :return d dict: dizionario {col:entropy}\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        s=pd.DataFrame(curr_user.groupby([col])[col2].sum())\n",
    "        s['tot']=[sum(s[col2])]*len(s)\n",
    "        num_classi=0\n",
    "        if col=='giorno':\n",
    "            num_classi=31\n",
    "        elif col=='mese':\n",
    "            num_classi=12\n",
    "        elif col=='ora':\n",
    "            num_classi=14\n",
    "        elif col=='giorno_settimana':\n",
    "            num_classi=7\n",
    "        elif col=='settimana_anno':\n",
    "            num_classi=53\n",
    "        return {col:(-1*sum(s[col2]/s['tot']*np.log(s[col2]/s['tot']))/np.log(num_classi))}\n",
    "\n",
    "    \n",
    "    import pandas as pd\n",
    "    import datetime\n",
    "    import numpy as np\n",
    "    \n",
    "    input_file='../data/dataset_shop70_coopometro_temporal_part_filtered_iqr.csv.gz'\n",
    "    df=pd.read_csv(input_file, compression='gzip',sep=';')\n",
    "    f_iqr=open('../data/dataset_shop70_coopometro_temporal_part_filtered_iqr_wheighted_entropy_%s_%s.csv' %(col2,i),'a')\n",
    "\n",
    "    \n",
    "    \n",
    "    cols=list(df.columns[3:-3]) #attenzione al numero di colonne che si hanno nel df, non ciclare sulle misure importo media e quantita\n",
    "    i=0\n",
    "    print \"%s: start\" %(datetime.datetime.now())\n",
    "    for cus in users:\n",
    "        df_entropy=pd.DataFrame()\n",
    "        curr_user=df[df['customer_id']==cus].copy()\n",
    "        i+=1\n",
    "        if i%1000==0:\n",
    "            print \"%s: performed %s users\" %(datetime.datetime.now(),i)\n",
    "\n",
    "        for anno in np.arange(2007,2015):\n",
    "            dfs=[]\n",
    "            for col in cols:\n",
    "                dfs.append(pd.DataFrame.from_dict({cus:get_wheighted_entropy(curr_user[curr_user['anno']==anno],col,col2)},orient='index'))\n",
    "            dfs.append(pd.DataFrame.from_dict({cus:{'anno':anno}},orient='index')) #join dei df dello stesso anno \n",
    "            df_entropy=df_entropy.append(pd.concat(dfs,axis=1)) #aggiunta del df dell'anno corrente agli altri anni\n",
    "    \n",
    "        #salvo il dataframe su file\n",
    "        df_entropy=df_entropy.reset_index()\n",
    "        df_entropy.to_csv(f_iqr, sep=';',header=False,index=False)\n",
    "    \n",
    "    f_iqr.flush()\n",
    "    f_iqr.close()\n",
    "\n",
    "    return i\n",
    "\n",
    "    \n",
    "#creazione di tanti file di n elementi max\n",
    "def chunks(l, n):\n",
    "    n = max(1, n)\n",
    "    return [l[i:i + n] for i in range(0, len(l), n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5 Entropia su pesata sulla quantita\n",
    "#run del processo\n",
    "client = Client()\n",
    "view = client.load_balanced_view()\n",
    "users=df_iqr['customer_id'].unique()\n",
    "users_=chunks(users,len(users)/3+1)\n",
    "results = parallel_plot(view, users_,'importo') #una volta per quantita una per importo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5 Entropia su pesata sulla quantita - distribuzioni\n",
    "import pandas\n",
    "for col1 in ['quantita','importo']:\n",
    "    infile='../data/dataset_shop70_coopometro_temporal_part_filtered_iqr.w_entropia_%s.txt' %(col1)\n",
    "    df_iqr_w_entropy=pd.read_csv(infile,sep=';',decimal='.')\n",
    "    df_iqr_w_entropy=df_iqr_w_entropy.replace(-0.0,0)\n",
    "    df_iqr_w_entropy=df_iqr_w_entropy.replace(-float('infinity'),0)\n",
    "    #statistica sulla entropia\n",
    "    import pylab\n",
    "    path='./'\n",
    "    #tolgo i casi in cui ho zero in tutte le entropie perchè non avevo dati\n",
    "    df_iqr_w_entropy=df_iqr_w_entropy[(df_iqr_w_entropy['mese']<>0) & (df_iqr_w_entropy['giorno']<>0)& (df_iqr_w_entropy['ora']<>0)& (df_iqr_w_entropy['giorno_settimana']<>0) & (df_iqr_w_entropy['settimana_anno']<>0) ]\n",
    "    for col in list(df_iqr_w_entropy.columns)[1:-1]:\n",
    "        df_iqr_w_entropy[col].hist()\n",
    "        pylab.xticks(rotation=0)\n",
    "        pylab.title(col,fontsize=20)\n",
    "        pylab.xlabel(\"entropy\", fontsize=16)\n",
    "        pylab.ylabel(\"#users\",fontsize=16)\n",
    "        pylab.savefig(\"%s%s_entropy_w_%s.png\" %(path,col,col1))\n",
    "        pylab.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#6 disegno traiettorie\n",
    "# sql example http://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html\n",
    "import numpy as np\n",
    "df_iqr_spesa_mensile=df_iqr.groupby(['customer_id','anno']).agg({'quantita': [np.size, np.mean],'importo': [np.size, np.mean]})\n",
    "\n",
    "import pylab\n",
    "dt=pd.DataFrame(df_iqr_spesa_mensile['importo']['mean']).reset_index()\n",
    "dt=pd.DataFrame(pd.pivot_table(dt,values='mean',index='customer_id',columns=['anno'])).replace('NaN',0)\n",
    "dt=dt.head(5000).T\n",
    "dt.plot(kind='line',alpha=0.01,color='b')\n",
    "pylab.legend(['traiettorie'])\n",
    "pylab.xlabel('anno',fontsize=16)\n",
    "pylab.ylabel('mediana acquisti annuali', fontsize=16)\n",
    "pylab.title('')\n",
    "pylab.ylim(0,200)\n",
    "pylab.savefig('../experiments/20151130_analisi_esplorativa/traiettorie.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#7 Clustering traiettorie\n",
    "import numpy as np\n",
    "#calcolo della media della spesa per mese\n",
    "df_iqr_spesa_mensile=df_iqr.groupby(['customer_id','anno']).agg({'quantita': [np.size, np.mean],'importo': [np.size, np.mean]})\n",
    "\n",
    "#pivot table id:<anni>\n",
    "import pylab\n",
    "dt=pd.DataFrame(df_iqr_spesa_mensile['importo']['mean']).reset_index()\n",
    "dt=pd.DataFrame(pd.pivot_table(dt,values='mean',index='customer_id',columns=['anno'])).replace('NaN',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#7 clustering di traiettorie annuali\n",
    "from sklearn.cluster import KMeans\n",
    "dataset=dt\n",
    "mat = dataset.as_matrix()\n",
    "km = KMeans(n_clusters=10)\n",
    "km.fit(mat)\n",
    "labels = km.labels_\n",
    "# Format results as a DataFrame\n",
    "results = pd.DataFrame([dataset.index,labels]).T\n",
    "results.columns=['customer_id','id_cluster']\n",
    "results.index=results.customer_id\n",
    "del(results['customer_id'])\n",
    "dataset=pd.concat([dataset,results],axis='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#disegno del cluster\n",
    "colors=[ '#1f78b4', '#b2df8a', '#fb9a99', '#e31a1c', '#33a02c', '#fdbf6f', '#ff7f00', '#cab2d6', '#6a3d9a', '#ffff99', '#a6cee3']\n",
    "centers=pd.DataFrame(km.cluster_centers_)\n",
    "centers.columns=np.arange(2007,2015)\n",
    "centers.T.plot(kind='line', colors=colors,lw=3)\n",
    "pylab.xticks(np.arange(2007,2015))\n",
    "pylab.ylabel('media mensile', fontsize=16)\n",
    "pylab.show()\n",
    "\n",
    "#salvo il risultato del cluster assieme alle figure\n",
    "dataset.groupby('id_cluster').count().to_csv('../experiments/20151130_analisi_esplorativa/traiettorie.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
